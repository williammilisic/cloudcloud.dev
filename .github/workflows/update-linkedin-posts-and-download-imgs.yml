name: Update LinkedIn Posts and Download Images

on:
  schedule:
    - cron: "0 0 * * 1" # Runs every Monday at midnight UTC
  workflow_dispatch:

jobs:
  update-linkedin-posts:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v3

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Fetch ALL LinkedIn posts with pagination
        env:
          RAPIDAPI_LINKEDIN_DATA_API_KEY: ${{ secrets.RAPIDAPI_LINKEDIN_DATA_API_KEY }}
        run: |
          posts_file="linkedin-posts.json"
          username="williammilisic"
          api_url="https://linkedin-scraper-api-real-time-fast-affordable.p.rapidapi.com/profile/posts"
          api_host="linkedin-scraper-api-real-time-fast-affordable.p.rapidapi.com"
          api_key="${RAPIDAPI_LINKEDIN_DATA_API_KEY}"

          # Temporary file to collect posts
          tmp_posts="tmp_posts.jsonl"
          : > "$tmp_posts"

          pagination_token=""
          while :; do
            if [ -z "$pagination_token" ]; then
              response=$(curl --silent --request GET \
                --url "$api_url?username=$username" \
                --header "x-rapidapi-host: $api_host" \
                --header "x-rapidapi-key: $api_key")
            else
              response=$(curl --silent --request GET \
                --url "$api_url?username=$username&paginationToken=$pagination_token" \
                --header "x-rapidapi-host: $api_host" \
                --header "x-rapidapi-key: $api_key")
            fi

            # Extract posts and append them as ndjson
            echo "$response" | jq -c '.data.posts[]?' >> "$tmp_posts"

            # Get next pagination token, if any
            pagination_token=$(echo "$response" | jq -r '.paginationToken // empty')

            # Stop if no more pages
            [ -z "$pagination_token" ] && break
          done

          # Combine all individual posts into a single JSON array
          jq -s '{success: true, message: "", data: {posts: .}}' "$tmp_posts" > "$posts_file"
          rm -f "$tmp_posts"

      - name: Merge new posts with existing JSON
        run: |
          if [ -f _data/linkedin-posts.json ]; then
            originalCount=$(jq '.data.posts | length' _data/linkedin-posts.json)
            jq -s '{
              success: true,
              message: "",
              data: {
                posts: (.[0].data.posts + .[1].data.posts
                  | unique_by(.posted_at.timestamp)
                  | sort_by(.posted_at.timestamp)
                  | reverse
                  | map(
                      .commentsCount = (.stats.comments | tonumber)
                      | .totalReactionCount = (.stats
